import numpy as np
import matplotlib.pyplot as plt

train = [0.6277056277056277, 0.6753246753246753, 0.7316017316017316, 0.8614718614718615, 0.8268398268398268,
         0.7705627705627706, 0.7619047619047619, 0.8441558441558441, 0.8181818181818182, 0.8484848484848485,
         0.8311688311688312, 0.8225108225108225, 0.8354978354978355, 0.8528138528138528, 0.8354978354978355,
         0.8528138528138528, 0.8528138528138528, 0.8614718614718615, 0.8484848484848485, 0.8787878787878788,
         0.8614718614718615, 0.8744588744588745, 0.8354978354978355, 0.8658008658008658, 0.8614718614718615,
         0.8528138528138528, 0.8744588744588745, 0.8874458874458875, 0.8874458874458875, 0.8701298701298701,
         0.8917748917748918, 0.8701298701298701, 0.8961038961038961, 0.9090909090909091, 0.8917748917748918,
         0.9004329004329005, 0.8961038961038961, 0.9134199134199135, 0.9047619047619048, 0.9090909090909091,
         0.9047619047619048, 0.9177489177489178, 0.9090909090909091, 0.9090909090909091, 0.9177489177489178,
         0.9307359307359307, 0.922077922077922, 0.9134199134199135, 0.9264069264069265, 0.9134199134199135,
         0.9134199134199135, 0.9177489177489178, 0.9307359307359307, 0.9264069264069265, 0.9177489177489178,
         0.935064935064935, 0.9177489177489178, 0.9307359307359307, 0.9437229437229437, 0.935064935064935,
         0.9307359307359307, 0.935064935064935, 0.9393939393939394, 0.9307359307359307, 0.948051948051948,
         0.9264069264069265, 0.9393939393939394, 0.9437229437229437, 0.9393939393939394, 0.9393939393939394,
         0.922077922077922, 0.9437229437229437, 0.9307359307359307, 0.9437229437229437, 0.9437229437229437,
         0.9393939393939394, 0.9523809523809523, 0.9567099567099567, 0.9523809523809523, 0.9437229437229437,
         0.9437229437229437, 0.948051948051948, 0.948051948051948, 0.9523809523809523, 0.9523809523809523,
         0.9523809523809523, 0.948051948051948, 0.9523809523809523, 0.9523809523809523, 0.961038961038961,
         0.9523809523809523, 0.9567099567099567, 0.9523809523809523, 0.961038961038961, 0.9567099567099567,
         0.961038961038961, 0.9696969696969697, 0.9696969696969697, 0.9696969696969697, 0.9653679653679653,
         0.9653679653679653, 0.974025974025974, 0.9696969696969697, 0.9696969696969697, 0.9653679653679653,
         0.974025974025974, 0.9783549783549783, 0.9696969696969697, 0.974025974025974, 0.987012987012987,
         0.987012987012987, 0.9826839826839827, 0.974025974025974, 0.987012987012987, 0.9826839826839827,
         0.987012987012987, 0.987012987012987, 0.987012987012987, 0.987012987012987, 0.987012987012987,
         0.987012987012987, 0.987012987012987, 0.987012987012987, 0.987012987012987, 0.987012987012987,
         0.987012987012987, 0.987012987012987, 0.987012987012987, 0.987012987012987, 0.987012987012987,
         0.987012987012987, 0.987012987012987, 0.987012987012987, 0.987012987012987, 0.987012987012987,
         0.987012987012987, 0.987012987012987, 0.987012987012987, 0.987012987012987, 0.987012987012987,
         0.987012987012987, 0.987012987012987, 0.987012987012987, 0.987012987012987, 0.987012987012987,
         0.987012987012987, 0.987012987012987, 0.987012987012987, 0.9913419913419913, 0.987012987012987,
         0.987012987012987, 0.9913419913419913, 0.987012987012987, 0.987012987012987, 0.9956709956709957,
         0.987012987012987, 0.9913419913419913, 0.9913419913419913, 0.987012987012987, 0.987012987012987,
         0.987012987012987, 0.987012987012987, 0.9913419913419913, 0.987012987012987, 0.9913419913419913,
         0.9913419913419913, 0.9956709956709957, 0.9913419913419913, 0.9913419913419913, 0.9956709956709957,
         0.9956709956709957, 0.9956709956709957, 0.9956709956709957, 0.9956709956709957, 0.9956709956709957,
         0.9956709956709957, 0.9956709956709957, 0.9956709956709957, 0.9956709956709957, 1.0, 0.9956709956709957,
         0.9956709956709957, 1.0, 0.9956709956709957, 0.9956709956709957, 1.0, 0.9956709956709957, 1.0, 1.0, 1.0, 1.0,
         1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
val = [0.696969696969697, 0.6868686868686869, 0.7272727272727273, 0.8383838383838383, 0.8282828282828283,
       0.8080808080808081, 0.8383838383838383, 0.8585858585858586, 0.8585858585858586, 0.8686868686868687,
       0.8282828282828283, 0.8484848484848485, 0.8282828282828283, 0.8787878787878788, 0.8282828282828283,
       0.8787878787878788, 0.8484848484848485, 0.8585858585858586, 0.8585858585858586, 0.8787878787878788,
       0.8686868686868687, 0.8888888888888888, 0.8787878787878788, 0.8787878787878788, 0.8686868686868687,
       0.8585858585858586, 0.8888888888888888, 0.8484848484848485, 0.8585858585858586, 0.8484848484848485,
       0.8787878787878788, 0.8383838383838383, 0.8686868686868687, 0.8787878787878788, 0.8787878787878788,
       0.8484848484848485, 0.8686868686868687, 0.8888888888888888, 0.8484848484848485, 0.9090909090909091,
       0.8686868686868687, 0.8686868686868687, 0.8888888888888888, 0.8888888888888888, 0.8686868686868687,
       0.9090909090909091, 0.8787878787878788, 0.8787878787878788, 0.8787878787878788, 0.8888888888888888,
       0.8787878787878788, 0.8888888888888888, 0.9090909090909091, 0.8888888888888888, 0.8585858585858586,
       0.8787878787878788, 0.8888888888888888, 0.9090909090909091, 0.9090909090909091, 0.8888888888888888,
       0.9090909090909091, 0.9090909090909091, 0.8888888888888888, 0.8888888888888888, 0.9191919191919192,
       0.898989898989899, 0.898989898989899, 0.9090909090909091, 0.9090909090909091, 0.9090909090909091,
       0.898989898989899, 0.9191919191919192, 0.9090909090909091, 0.9090909090909091, 0.898989898989899,
       0.9191919191919192, 0.9393939393939394, 0.9292929292929293, 0.9090909090909091, 0.9292929292929293,
       0.9191919191919192, 0.9494949494949495, 0.9292929292929293, 0.9191919191919192, 0.9292929292929293,
       0.9191919191919192, 0.9191919191919192, 0.9292929292929293, 0.9090909090909091, 0.9292929292929293,
       0.9292929292929293, 0.9191919191919192, 0.9292929292929293, 0.9191919191919192, 0.9191919191919192,
       0.9292929292929293, 0.9292929292929293, 0.9292929292929293, 0.9292929292929293, 0.9191919191919192,
       0.9393939393939394, 0.9191919191919192, 0.9191919191919192, 0.9292929292929293, 0.9292929292929293,
       0.9292929292929293, 0.9292929292929293, 0.9191919191919192, 0.9292929292929293, 0.9292929292929293,
       0.9292929292929293, 0.9292929292929293, 0.9292929292929293, 0.9292929292929293, 0.9292929292929293,
       0.9292929292929293, 0.9292929292929293, 0.9292929292929293, 0.9191919191919192, 0.9292929292929293,
       0.9393939393939394, 0.9191919191919192, 0.9191919191919192, 0.9191919191919192, 0.9191919191919192,
       0.9292929292929293, 0.9191919191919192, 0.9191919191919192, 0.9191919191919192, 0.9292929292929293,
       0.9191919191919192, 0.9090909090909091, 0.9191919191919192, 0.9191919191919192, 0.9090909090909091,
       0.9090909090909091, 0.9292929292929293, 0.9191919191919192, 0.9191919191919192, 0.9191919191919192,
       0.9090909090909091, 0.8888888888888888, 0.9191919191919192, 0.9090909090909091, 0.8888888888888888,
       0.898989898989899, 0.898989898989899, 0.8888888888888888, 0.9090909090909091, 0.898989898989899,
       0.898989898989899, 0.9191919191919192, 0.8888888888888888, 0.9191919191919192, 0.898989898989899,
       0.8888888888888888, 0.9090909090909091, 0.9090909090909091, 0.898989898989899, 0.8888888888888888,
       0.8787878787878788, 0.8787878787878788, 0.9090909090909091, 0.8787878787878788, 0.9090909090909091,
       0.8787878787878788, 0.9090909090909091, 0.9090909090909091, 0.9090909090909091, 0.898989898989899,
       0.9090909090909091, 0.898989898989899, 0.9090909090909091, 0.9191919191919192, 0.9292929292929293,
       0.9090909090909091, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888,
       0.9090909090909091, 0.8888888888888888, 0.898989898989899, 0.8888888888888888, 0.8787878787878788,
       0.9090909090909091, 0.8787878787878788, 0.8888888888888888, 0.9090909090909091, 0.8888888888888888,
       0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8686868686868687, 0.8686868686868687,
       0.8686868686868687, 0.8686868686868687, 0.8686868686868687, 0.8686868686868687]

print(val.index(max(val)))
print(max(val))
print(val[81])
print(train[81])
x_data = range(1, 200)

plt.figure(dpi=600)

ln1, = plt.plot(x_data, train, color='red', linewidth=2.0, linestyle='-')
ln2, = plt.plot(x_data, val, color='blue', linewidth=3.0, linestyle='-')

plt.title("Accuracy of training and test sets with different number of neurons")  # 设置标题及字体

plt.legend(handles=[ln1, ln2], labels=['train', 'validation'])

plt.xlabel('Number of neurons')
plt.ylabel('Accuracy')

plt.xlim(xmin=-10,xmax=210)
plt.ylim(ymin=0.60,ymax=1.04)

plt.scatter(81, 0.9494949494949495, s=20, color='g')
plt.plot([81,81],[0.9494949494949495,0.60],'g--',lw=2)
plt.plot([-10,81],[0.9494949494949495,0.9494949494949495],'g--',lw=2)

ax = plt.gca()
ax.spines['right'].set_color('none')  # right边框属性设置为none 不显示
ax.spines['top'].set_color('none')  # top边框属性设置为none 不显示

plt.show()
